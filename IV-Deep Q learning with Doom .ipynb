{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 13:38:06.287746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-22 13:38:06.299118: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-22 13:38:06.302704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-22 13:38:06.312910: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-22 13:38:07.061750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf      \n",
    "import numpy as np           \n",
    "from vizdoom import *        \n",
    "\n",
    "import random                \n",
    "import time                  \n",
    "from skimage import transform\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Desactivar la ejecución ansiosa\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Restablecer el gráfico\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar Ambiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "\n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            time.sleep(0.02)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pre procesamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_frame(frame):\n",
    "    # Verifica si hay NaN en el frame y reemplázalos por 0\n",
    "    if np.isnan(frame).any():\n",
    "        frame = np.nan_to_num(frame, nan=0.0)\n",
    "\n",
    "    # Verifica que la imagen tenga dimensiones adecuadas (2D o 3D)\n",
    "    if frame.ndim == 3:\n",
    "        frame = frame[..., 0]  # Si tiene más de 2 dimensiones, selecciona un solo canal\n",
    "\n",
    "    # Verifica que el frame tenga el tamaño esperado\n",
    "    if frame.shape[0] < 40 or frame.shape[1] < 40:\n",
    "        print(\"La imagen de entrada es demasiado pequeña para procesar\")\n",
    "        return np.zeros((84, 84)) \n",
    "\n",
    "    # Recortar la pantalla (ajustar para quitar partes irrelevantes)\n",
    "    cropped_frame = frame[30:-10, 30:-30]\n",
    "\n",
    "    # Normalizar los valores de los píxeles\n",
    "    normalized_frame = cropped_frame / 255.0\n",
    "\n",
    "    # Verificar si hay NaN después de la normalización\n",
    "    if np.isnan(normalized_frame).any():\n",
    "        normalized_frame = np.nan_to_num(normalized_frame, nan=0.0)\n",
    "\n",
    "    # Asegurar que el frame tenga dimensiones válidas antes del redimensionado\n",
    "    if normalized_frame.size == 0 or normalized_frame.shape[0] == 0 or normalized_frame.shape[1] == 0:\n",
    "        raise ValueError(\"La imagen después de la normalización está vacía o tiene dimensiones no válidas\")\n",
    "\n",
    "    # Redimensionar el frame a 84x84 usando antialiasing\n",
    "    preprocessed_frame = transform.resize(normalized_frame, (84, 84), anti_aliasing=True)\n",
    "    print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "\n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APILAR FRAMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir nuestra Deep Q-learning Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DQNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        super(DQNetwork, self).__init__(name=name)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Definir capas\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[8, 8],\n",
    "            strides=[4, 4],\n",
    "            padding=\"valid\",\n",
    "            kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            name=\"conv1\"\n",
    "        )\n",
    "        \n",
    "        self.conv1_batchnorm = tf.keras.layers.BatchNormalization(\n",
    "            epsilon=1e-5,\n",
    "            name='batch_norm1'\n",
    "        )\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"valid\",\n",
    "            kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            name=\"conv2\"\n",
    "        )\n",
    "        \n",
    "        self.conv2_batchnorm = tf.keras.layers.BatchNormalization(\n",
    "            epsilon=1e-5,\n",
    "            name='batch_norm2'\n",
    "        )\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"valid\",\n",
    "            kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            name=\"conv3\"\n",
    "        )\n",
    "\n",
    "        self.conv3_batchnorm = tf.keras.layers.BatchNormalization(\n",
    "            epsilon=1e-5,\n",
    "            name='batch_norm3'\n",
    "        )\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(\n",
    "            units=512,\n",
    "            activation=tf.nn.elu,\n",
    "            kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=3, \n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            name=\"output\"\n",
    "        )\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Flujo de datos a través de las capas\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv1_batchnorm(x, training=training)\n",
    "        x = tf.nn.elu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_batchnorm(x, training=training)\n",
    "        x = tf.nn.elu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.conv3_batchnorm(x, training=training)\n",
    "        x = tf.nn.elu(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def compute_loss(self, actions, target_Q, output):\n",
    "        # Q es el valor Q predicho\n",
    "        Q = tf.reduce_sum(tf.multiply(output, actions), axis=1)\n",
    "        \n",
    "        # La pérdida es la diferencia entre nuestros valores Q predichos y Q_target\n",
    "        loss = tf.reduce_mean(tf.square(target_Q - Q))\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs, actions, target_Q):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(inputs, training=True)\n",
    "            loss = self.compute_loss(actions, target_Q, output)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "#tf.reset_default_graph()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir priorizaciónm de experiencias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La imagen de entrada es demasiado pequeña para procesar\n"
     ]
    }
   ],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        print(f\"Dimensiones del estado: {state.shape}\")  # Esto imprimirá las dimensiones del estado\n",
    "\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With ϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        #Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        Qs = DQNetwork(np.expand_dims(state, axis=0))  # Usamos el modelo directamente\n",
    "\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n",
      "La imagen de entrada es demasiado pequeña para procesar\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic tf.Tensor (DQNetwork_1/output_1/Add:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m decay_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Aumenta el contador para reducir epsilon en la exploración\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Predecir la acción a tomar\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m action, explore_probability \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexplore_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Tomar la acción en el juego\u001b[39;00m\n\u001b[1;32m     25\u001b[0m reward \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mmake_action(action)\n",
      "Cell \u001b[0;32mIn[26], line 26\u001b[0m, in \u001b[0;36mpredict_action\u001b[0;34m(explore_start, explore_stop, decay_rate, decay_step, state, actions)\u001b[0m\n\u001b[1;32m     22\u001b[0m     Qs \u001b[38;5;241m=\u001b[39m DQNetwork(np\u001b[38;5;241m.\u001b[39mexpand_dims(state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# Usamos el modelo directamente\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Take the biggest Q value (= the best action)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     choice \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     action \u001b[38;5;241m=\u001b[39m possible_actions[\u001b[38;5;28mint\u001b[39m(choice)]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, explore_probability\n",
      "File \u001b[0;32m~/PycharmProjects/venv312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/venv312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/PycharmProjects/venv312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m~/PycharmProjects/venv312/lib/python3.12/site-packages/tensorflow/python/framework/tensor.py:627\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    626\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m dtype\n\u001b[0;32m--> 627\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    628\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert a symbolic tf.Tensor (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) to a numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This error may indicate that you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre trying to pass a Tensor to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a NumPy call, which is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic tf.Tensor (DQNetwork_1/output_1/Add:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported."
     ]
    }
   ],
   "source": [
    "# Inicializar decay_step al principio del entrenamiento\n",
    "decay_step = 0\n",
    "\n",
    "# Si estás en modo de entrenamiento\n",
    "if training:\n",
    "    for episode in range(total_episodes):\n",
    "        step = 0\n",
    "        episode_rewards = []\n",
    "\n",
    "        # Iniciar un nuevo episodio\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "\n",
    "        # Apilar los frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        while step < max_steps:\n",
    "            step += 1\n",
    "            decay_step += 1  # Aumenta el contador para reducir epsilon en la exploración\n",
    "\n",
    "            # Predecir la acción a tomar\n",
    "            action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "            # Tomar la acción en el juego\n",
    "            reward = game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                next_state = np.zeros((84, 84), dtype=int)\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                total_reward = np.sum(episode_rewards)\n",
    "                print(f'Episodio: {episode}, Recompensa total: {total_reward}, Pérdida: {loss:.4f}, Probabilidad de exploración: {explore_probability:.4f}')\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "                step = max_steps\n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "            # Aprendizaje...\n",
    "            # (El resto de tu código)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
